## @section Percona Monitoring and Management (PMM) parameters
## Default values for PMM.
## This is a YAML-formatted file.
## Declare variables to be passed into your templates.

## PMM image version
## ref: https://hub.docker.com/r/percona/pmm-server/tags
## @param image.repository PMM image repository
## @param image.pullPolicy PMM image pull policy
## @param image.tag PMM image tag
## @param image.imagePullSecrets Global Docker registry secret names as an array
##
image:
  repository: percona/pmm-server
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "3.6.0"
  imagePullSecrets: []

## PMM environment variables
## ref: https://docs.percona.com/percona-monitoring-and-management/3/install-pmm/install-pmm-server/deployment-options/docker/env_var.html
##
pmmEnv:
  ## @param pmmEnv.PMM_ENABLE_UPDATES needs to be disabled in k8s environment as updates are rolled out via helm/container update
  ##
  PMM_ENABLE_UPDATES: "0"
  PMM_DEBUG: "0"
  PMM_DISABLE_BUILTIN_CLICKHOUSE: "1"
  PMM_DISABLE_BUILTIN_POSTGRES: "1"
  # PMM_CLICKHOUSE_ADDR: "{{ .Release.Name }}-clickhouse.{{ .Release.Namespace }}.svc.cluster.local:9000"
  PMM_CLICKHOUSE_DATABASE: "pmm"
  PMM_CLICKHOUSE_IS_CLUSTER: "1"
  # PMM_CLICKHOUSE_CLUSTER_NAME is set directly in statefulset.yaml
  # PMM_CLICKHOUSE_USER: "clickhouse_pmm"
  # PMM_CLICKHOUSE_PASSWORD: "chpass"
  # PMM_POSTGRES_ADDR: "pmmhak8s-pg-db-ha.pmmhak8s.svc.cluster.local:5432"
  PMM_POSTGRES_SSL_MODE: "require"
  # GF_DATABASE_URL: "postgres://$(GF_USERNAME):$(GF_PASSWORD)@:5432/grafana"
  GF_DATABASE_SSL_MODE: "require"
  # PMM_VM_URL: "http://:@:8427/"
  # VMAGENT_remoteWrite_basicAuth_username: ""
  # VMAGENT_remoteWrite_basicAuth_password: ""
  PMM_HA_ENABLE: "1"
  PMM_HA_GOSSIP_PORT: "9096"
  PMM_HA_RAFT_PORT: "9097"
  PMM_HA_GRAFANA_GOSSIP_PORT: "9094"
  # PMM_HA_PEERS: "{{ include "pmm.haPeers" . }}"
  GF_ANALYTICS_CHECK_FOR_UPDATES: "0"
  GF_ANALYTICS_ENABLE: "0"
  GF_ANALYTICS_REPORTING_ENABLE: "0"
  GF_REPORTING_ENABLE: "0"
  GF_NEWS_NEWS_FEED_ENABLE: "0"
  GF_SECURITY_DISABLE_GRAVATAR: "1"
  GF_LOG_LEVEL: "info"
  PMM_ENABLE_TELEMETRY: "0"
  PMM_ENABLE_BACKUP_MANAGEMENT: "1"
  PMM_ENABLE_ALERTING: "1"
  PMM_ENABLE_ACCESS_CONTROL: "1"

#  optional variables to integrate Grafana with internal iDP, see also secret part
#  GF_AUTH_GENERIC_OAUTH_ENABLED: "1"
#  GF_AUTH_GENERIC_OAUTH_SCOPES: ''
#  GF_AUTH_GENERIC_OAUTH_AUTH_URL: ''
#  GF_AUTH_GENERIC_OAUTH_TOKEN_URL: ''
#  GF_AUTH_GENERIC_OAUTH_API_URL: ''
#  GF_AUTH_GENERIC_OAUTH_ALLOWED_DOMAINS: ''

## @param pmmResources optional [Resources](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) 
## requested for [PMM container](https://docs.percona.com/percona-monitoring-and-management/3/install-pmm/install-pmm-server/deployment-options/docker/index.html)
pmmResources:
  # PMM server resource configuration
  requests:
    memory: "3Gi"
    cpu: "2"
  limits:
    memory: "4Gi"
    cpu: "2"
# pmmResources: {}

## Readiness probe Config
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
## @param readyProbeConf.initialDelaySeconds  Number of seconds after the container has started before readiness probes is initiated
## @param readyProbeConf.periodSeconds How often (in seconds) to perform the probe
## @param readyProbeConf.failureThreshold When a probe fails, Kubernetes will try failureThreshold times before giving up
##
readyProbeConf:
  initialDelaySeconds: 10
  periodSeconds: 5
  failureThreshold: 6

## Log streamer sidecar configuration
## Streams log files to stdout for kubectl logs access
## Each log file gets its own sidecar container named log-streamer-<index>
## @param logStreamer.enabled Enable log streamer sidecar containers
## @param logStreamer.logFiles List of log file paths to stream
##
logStreamer:
  enabled: false
  logFiles:
    - /srv/logs/pmm-managed.log
    - /srv/logs/qan-api2.log

## @section PMM secrets
##
secret:
  ## @param secret.name Defines the name of the k8s secret that holds passwords and other secrets
  ##
  name: pmm-secret
  ## @param secret.annotations -- Secret annotations configuration
  annotations: {}
  ## @param secret.create If true then secret will be generated by Helm chart. Otherwise it is expected to be created by user.
  ##
  create: false
  ## @param secret.pmm_password Initial PMM password - it changes only on the first deployment, ignored if PMM was already provisioned. 
  ## If PMM admin password is not set, it will be generated.
  ## E.g.
  ## pmm_password: admin
  ##
  ## To get password execute `kubectl get secret pmm-secret -o jsonpath='{.data.PMM_ADMIN_PASSWORD}' | base64 --decode`
  ##
  pmm_password: ""
  ##
  ## @param secret.clickhouse_user ClickHouse user. If not set, defaults to "clickhouse_pmm"
  ## When secret.create: true and this is empty, "clickhouse_pmm" is used.
  ## If creating the secret manually (secret.create: false), this field is ignored.
  ##
  ## To get username after install:
  ##   kubectl get secret pmm-secret -o jsonpath='{.data.PMM_CLICKHOUSE_USER}' | base64 --decode
  ## E.g.
  ## clickhouse_user: clickhouse_user
  ##
  clickhouse_user: ""
  ##
  ## @param secret.clickhouse_password ClickHouse password. If not set, it will be auto-generated
  ## When secret.create: true and this is empty, a random 32-character password is generated.
  ## If creating the secret manually (secret.create: false), this field is ignored.
  ##
  ## To get password after install:
  ##   kubectl get secret pmm-secret -o jsonpath='{.data.PMM_CLICKHOUSE_PASSWORD}' | base64 --decode
  ## E.g.
  ## clickhouse_password: my_secure_password
  ##
  clickhouse_password: ""
  ##
  ## @param secret.victoriametrics_user VictoriaMetrics user. If not set, defaults to "victoriametrics_pmm"
  ## When secret.create: true and this is empty, "victoriametrics_pmm" is used.
  ## If creating the secret manually (secret.create: false), this field is ignored.
  ##
  ## To get username after install:
  ##   kubectl get secret pmm-secret -o jsonpath='{.data.VMAGENT_remoteWrite_basicAuth_username}' | base64 --decode
  ## E.g.
  ## victoriametrics_user: my_victoriametrics_user
  ##
  victoriametrics_user: ""
  ##
  ## @param secret.victoriametrics_password VictoriaMetrics password. If not set, it will be auto-generated
  ## When secret.create: true and this is empty, a random 32-character password is generated.
  ## If creating the secret manually (secret.create: false), this field is ignored.
  ##
  ## To get password after install:
  ##   kubectl get secret pmm-secret -o jsonpath='{.data.VMAGENT_remoteWrite_basicAuth_password}' | base64 --decode
  ## E.g.
  ## victoriametrics_password: my_secure_password
  ##
  victoriametrics_password: ""
  ##
  ## @param secret.pg_password PostgreSQL PMM user password. If not set, it will be auto-generated
  ## When secret.create: true and this is empty, a random 32-character password is generated.
  ## If creating the secret manually (secret.create: false), this field is ignored.
  ##
  ## To get password after install:
  ##   kubectl get secret pmm-secret -o jsonpath='{.data.PG_PASSWORD}' | base64 --decode
  ## E.g.
  ## pg_password: my_secure_password
  ##
  pg_password: ""
  ##
  ## @param secret.gf_password PostgreSQL Grafana user password. If not set, it will be auto-generated
  ## When secret.create: true and this is empty, a random 32-character password is generated.
  ## If creating the secret manually (secret.create: false), this field is ignored.
  ##
  ## To get password after install:
  ##   kubectl get secret pmm-secret -o jsonpath='{.data.GF_PASSWORD}' | base64 --decode
  ## E.g.
  ## gf_password: my_secure_password
  ##
  gf_password: ""
  ##
  # GF_AUTH_GENERIC_OAUTH_CLIENT_ID optional client ID to integrate Grafana with internal iDP, requires other env defined as well under pmmEnv
  # GF_AUTH_GENERIC_OAUTH_CLIENT_ID:
  # GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET optional secret to integrate Grafana with internal iDP, requires other env defined as well under pmmEnv
  # GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET:

## @param certs Optional certificates, if not provided PMM would use generated self-signed certificates,
##   please provide your own signed ssl certificates like this in base 64 format:
## certs:
  ## name: pmm-certs
  ## files:
  ##  certificate.crt:
  ##  certificate.key:
  ##  ca-certs.pem:
  ##  dhparam.pem:
  ##  certificate.conf:
certs: {}

## @section PMM network configuration
## Service configuration
##
service:
  ## @param service.name Service name that is dns name monitoring services would send data to. `monitoring-service` used by default by pmm-client in Percona operators.
  ##
  name: monitoring-service
  ## @param service.type Kubernetes Service type
  ##
  type: "ClusterIP"

  ## Ports 8443 for https
  ##
  ports:
    ## @param service.ports[0].port https port number
    - port: 8443
      ## @param service.ports[0].targetPort target port to map for statefulset and ingress
      targetPort: https
      ## @param service.ports[0].protocol protocol for https
      protocol: TCP
      ## @param service.ports[0].name port name
      name: https

replicas: 3

## @param maxReplicas Maximum number of PMM replicas for HAProxy server-template
## HAProxy uses DNS-based discovery with server-template. This value sets the
## maximum number of server slots HAProxy will create. Adjust if you plan to
## scale beyond 10 replicas.
maxReplicas: 10

## Ingress controller configuration
##
ingress:
  ## @param ingress.enabled -- Enable ingress controller resource
  enabled: false
  ## @param ingress.nginxInc -- Using ingress controller from NGINX Inc
  nginxInc: false
  ## @param ingress.annotations -- Ingress annotations configuration
  annotations: {}
    ## kubernetes.io/ingress.class: nginx
    ## kubernetes.io/tls-acme: "true"
    ###  nginx proxy to https
    ## nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
  ## @param ingress.community.annotations -- Ingress annotations configuration for community managed ingress (nginxInc = false)
  community:
    annotations: {}
      ## kubernetes.io/ingress.class: nginx
      ## kubernetes.io/tls-acme: "true"
  ## @param ingress.ingressClassName -- Sets the ingress controller class name to use.
  ingressClassName: ""

  ## Ingress resource hostnames and path mappings
  hosts:
    ## @param ingress.hosts[0].host hostname
    - host: chart-example.local
    ## @param ingress.hosts[0].paths path mapping
      paths: []

  ## @param ingress.pathType -- How ingress paths should be treated.
  pathType: Prefix

  ## @param ingress.tls -- Ingress TLS configuration
  tls: []
  ##  - secretName: chart-example-tls
  ##    hosts:
  ##      - chart-example.local

## @section PMM storage configuration
## Claiming storage for PMM using Persistent Volume Claims (PVC)
## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
##
storage:
  ## @param storage.name name of PVC
  name: pmm-storage
  ## @param storage.storageClassName optional PMM data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClassName: ""
  ##
  ## @param storage.size size of storage [depends](https://docs.percona.com/percona-monitoring-and-management/setting-up/server/index.html#set-up-pmm-server) on number of monitored services and data retention
  ##
  size: 40Gi
  ##
  ## @param storage.dataSource VolumeSnapshot to start from
  ##
  dataSource: {}
    ## name: before-vX.Y.Z-upgrade
    ## kind: VolumeSnapshot
    ## apiGroup: snapshot.storage.k8s.io
  ##
  ## @param storage.selector select existing PersistentVolume
  ##
  selector: {}
  ##   matchLabels:
  ##     release: "stable"
  ##   matchExpressions:
  ##     - key: environment
  ##       operator: In
  ##       values:
  ##         - dev

## @section PMM kubernetes configurations
## @param nameOverride String to partially override common.names.fullname template with a string (will prepend the release name)
##
nameOverride: ""

## @param extraLabels Labels to add to all deployed objects
##
extraLabels: {}

## Pods Service Account
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
## @param serviceAccount.create Specifies whether a ServiceAccount should be created
## @param serviceAccount.annotations Annotations for service account. Evaluated as a template. Only used if `create` is `true`.
## @param serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
##
serviceAccount:
  create: true
  annotations: {}
  name: "pmm-service-account"

## @param podAnnotations Pod annotations
## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
##
podAnnotations: {}

## @param podSecurityContext Configure Pods Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
## Note: we intentionally don't use the `runAsGroup` to make it compatible with OpenShift. 
## However, `runAsUser` needs to be set to nil for OpenShift to be able to assign a random user ID.
podSecurityContext:
  runAsUser: 1000
  fsGroup: 1000

## @param securityContext Configure Container Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
## securityContext.capabilities The capabilities to add/drop when running containers
## securityContext.runAsUser Set pmm containers' Security Context runAsUser
## securityContext.runAsNonRoot Set pmm container's Security Context runAsNonRoot
## E.g.
## securityContext:
  ## capabilities:
  ##   drop:
  ##   - ALL
  ## readOnlyRootFilesystem: true
  ## runAsNonRoot: true
  ## runAsUser: 1000
securityContext: {}


## @param nodeSelector Node labels for pod assignment
## Ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}

## @param tolerations Tolerations for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations: []

## @param affinity Affinity for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
##
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: pmm
          topologyKey: kubernetes.io/hostname

## @param extraVolumeMounts Optionally specify extra list of additional volumeMounts
##
extraVolumeMounts: []
## @param extraVolumes Optionally specify extra list of additional volumes
##
extraVolumes: []

clickhouse:
  # ClickHouse server image configuration
  image:
    repository: altinity/clickhouse-server
    tag: 25.3.6.10034.altinitystable-alpine
    pullPolicy: IfNotPresent
  
  # ClickHouse server resource configuration
  resources:
    requests:
      memory: "4Gi"
      cpu: "2"
    limits:
      memory: "8Gi"
      cpu: "4"
  
  cluster:
    # Cluster name restrictions: alphanumeric only, max 15 chars, no underscores
    name: pmmdb
    # Sharding and Replication setup
    shards: 1
    replicas: 3
  
  # Service configuration for sticky connections
  service:
    # Enable sticky service with session affinity
    enabled: true
    # Session affinity timeout in seconds (how long connections stay sticky)
    sessionAffinityTimeout: 10800  # 3 hours
  
  # ClickHouse Keeper configuration
  keeper:
    # ClickHouse Keeper image configuration
    image:
      repository: clickhouse/clickhouse-keeper
      tag: "25.3.6.56"
      pullPolicy: IfNotPresent
    # ClickHouse Keeper resource configuration
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "500m"
    # ClickHouse Keeper cluster configuration
    cluster:
      # Keeper cluster name (follows same restrictions as ClickHouse cluster)
      # Regex: ^[a-zA-Z0-9-]{0,15}$
      name: keeper
    # Number of Keeper nodes (should be odd number: 1, 3, 5, etc.)
    replicasCount: 3
    # Persistent storage configuration for Keeper data
    storage:
      # Size of persistent volume for each Keeper pod
      size: 5Gi
      # Storage class for Keeper persistent volumes
      # If empty, uses the cluster's default storage class
      storageClassName: ""
  
  # Persistent storage configuration for ClickHouse data
  storage:
    # Size of persistent volume for each ClickHouse pod
    size: 20Gi
    # Storage class for ClickHouse persistent volumes
    # If empty, uses the cluster's default storage class
    # storageClassName: ""
    storageClassName: ""
  
  # PMM application user configuration
  # User credentials are read from the pmm-secret at chart installation time
  users:
    # Network access configuration for ClickHouse user
    networks:
      ip:
        # Allow access from Kubernetes pod networks and HAProxy
        - 10.0.0.0/8
        - 172.16.0.0/12
        - 192.168.0.0/16

haproxy:
  # Override the HAProxy service name to be fixed (not include release name)
  # This allows pg-db.pmm.serverHost to use a consistent "pmm-ha-haproxy" value
  fullnameOverride: pmm-ha-haproxy
  config: ""
  configmap:
    enabled: false
  # HAProxy dependency configuration
  enabled: true
  # Number of HAProxy replicas
  replicaCount: 3
  
  ## HAProxy Service configuration for external access
  ## @param haproxy.service.type Service type for HAProxy
  ##   - ClusterIP: Internal only (default)
  ##   - LoadBalancer: External access via cloud/on-prem load balancer
  ##   - NodePort: External access via node ports
  ## @param haproxy.service.annotations Service annotations (add cloud-specific annotations as needed)
  ##
  service:
    type: ClusterIP
    ## Example annotations for different cloud providers:
    ##   AWS NLB: service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    ##   GCP Internal: cloud.google.com/load-balancer-type: "Internal"
    ##   Azure Internal: service.beta.kubernetes.io/azure-load-balancer-internal: "true"
    ## For MetalLB: metallb.universe.tf/address-pool: "production-public-ips"
    annotations: {}
  
  # Pod annotations
  # Note: HAProxy uses DNS-based discovery (server-template) for PMM backends,
  # so scaling up/down replicas does NOT require HAProxy restart.
  # Increment config-version only for other ConfigMap changes that require restart.
  podAnnotations:
    pmm.percona.com/config-version: "3"

  # Pod anti-affinity for HAProxy pods
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: haproxy
  # Init container to wait for PMM instances to be ready
  initContainers:
    - name: wait-for-pmm-ready
      image: "alpine:latest"
      command:
        - /bin/sh
        - /scripts/wait-for-pmm.sh
      volumeMounts:
        - name: init-script
          mountPath: /scripts
          readOnly: true
      resources:
        requests:
          cpu: 50m
          memory: 32Mi
        limits:
          cpu: 100m
          memory: 64Mi

  # Additional volumes for init container
  extraVolumes:
    - name: init-script
      configMap:
        name: pmm-ha-haproxy-init-script
        defaultMode: 0755
  # Disable default ConfigMap creation - we create our own
  configMap:
    enabled: false
  # Mount SSL certificate
  mountedSecrets:
    - volumeName: ssl-certificate
      secretName: haproxy-tls-secret
      mountPath: /etc/haproxy/certs
  
  # HAProxy monitoring and metrics configuration
  monitoring:
    # Enable HAProxy stats page
    stats:
      enabled: true
      port: 1024
      uri: /stats
      refresh: 30s
    # Enable Prometheus metrics export
    prometheus:
      enabled: true
      port: 1024
      path: /metrics
  
  # HAProxy security configuration
  security:
    # Enable HTTP to HTTPS redirect
    redirectToHttps: true

# HAProxy TLS configuration
tls:
  enabled: true
  # If true, skip secret creation entirely (you must manually create pemSecretName)
  # Set this to true if you want to manually manage haproxy-tls-secret
  manualSecret: false
  # If you have a k8s TLS secret (tls.crt/tls.key), Helm will convert it to PEM format
  # Leave empty to auto-generate self-signed certificate
  existingTLSSecret: ""            # e.g. "my-tls" (type=kubernetes.io/tls)
  # Name of the PEM secret that HAProxy will mount
  pemSecretName: "haproxy-tls-secret"
  cn: "pmm.local"
  dnsNames: 
    - "pmm.local"
    - "*.pmm.local" 
    - "localhost"
  validityDays: 1825  # 5 years
  # Force regeneration of the TLS secret (increment this value to force regeneration)
  forceRegenerate: 0

## @section PostgreSQL operator (pg-db) configuration
## @param pg-db Configuration values passed to the pg-db dependency chart
##
pg-db:
  autoCreateUserSchema: true
  
  # PostgreSQL user configuration
  # IMPORTANT: grafanaUserName and pmmUserName must match user names in the users array below
  # If you change a username here, you MUST also update the corresponding name in the users array
  grafanaUserName: "gfuser"    # Must match a user.name below
  pmmUserName: "pmmuser"       # Must match a user.name below
  
  # PostgreSQL users array (order doesn't matter - looked up by name)
  users:
    # Grafana user - name must match grafanaUserName above
    - name: gfuser
      databases:
        - grafana
        - pmm-managed
      options: "CREATEDB CREATEROLE"
      password:
        type: ASCII
      secretName: "gfuser-credentials"
    # PMM user - name must match pmmUserName above
    - name: pmmuser
      databases:
        - pmm-managed
        - grafana
      options: "CREATEDB CREATEROLE"
      password:
        type: ASCII
      secretName: "pmmuser-credentials"
  databaseInitSQL:
    key: init.sql
    name: postgresql-init-extensions
  instances:
    - name: instance1
      replicas: 3
      dataVolumeClaimSpec:
        # storageClassName: ""  # optional; inherit default if omitted
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  postgres-operator.crunchydata.com/data: postgres
                  postgres-operator.crunchydata.com/instance-set: instance1
  proxy:
    pgBouncer:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  postgres-operator.crunchydata.com/role: pgbouncer
  patroni:
    dynamicConfiguration:
      postgresql:
        pg_hba:
          # Allow connections from private RFC1918 networks (common Kubernetes ranges)
          # 10.0.0.0/8 covers AWS and GCP
          # 172.16.0.0/12 covers Azure and Docker
          # 192.168.0.0/16 covers minikube and many on-prem setups
          - host all gfuser 10.0.0.0/8 md5
          - host all pmmuser 10.0.0.0/8 md5
          - host all gfuser 172.16.0.0/12 md5
          - host all pmmuser 172.16.0.0/12 md5
          - host all gfuser 192.168.0.0/16 md5
          - host all pmmuser 192.168.0.0/16 md5
  
  ## @param pg-db.pmm PMM integration for PostgreSQL monitoring
  ## When enabled, PostgreSQL pods will push metrics directly to PMM Server
  ## A Job will automatically create the required service account token in PMM
  ## and store it in the specified secret for the PostgreSQL operator to use
  ##
  pmm:
    ## @param pg-db.pmm.enabled Enable PMM monitoring for PostgreSQL
    ## When true, a sidecar container will be added to PostgreSQL pods
    ## that pushes metrics to PMM Server
    enabled: true
    ## @param pg-db.pmm.image PMM client image configuration
    image:
      repository: docker.io/percona/pmm-client
      tag: "3"
    ## @param pg-db.pmm.secret Name of the secret containing PMM_SERVER_TOKEN
    ## This secret is automatically created by the pmm-token-init Job
    ## The secret must contain a key named PMM_SERVER_TOKEN with the service account token
    secret: pg-pmm-secret
    ## @param pg-db.pmm.serverHost PMM server hostname (via HAProxy for HA setup)
    ## Uses the fixed "pmm-ha-haproxy" service (HAProxy with fullnameOverride)
    serverHost: pmm-ha-haproxy
    ## @param pg-db.pmm.resources Resource requests and limits for PMM client sidecar
    # resources:
    #   requests:
    #     memory: 200M
    #     cpu: 500m

## @section Kubernetes cluster metrics (kube-state-metrics + node-exporter)
## Installs kube-state-metrics and node-exporter by default and lets the
## built-in VMAgent scrape them along with kubelet/cAdvisor and apiserver.
## ref: https://github.com/kubernetes/kube-state-metrics
## ref: https://github.com/prometheus/node_exporter
kube-state-metrics:
  enabled: true

prometheus-node-exporter:
  enabled: true


## @section VictoriaMetrics Configuration
## 
## IMPORTANT: The VictoriaMetrics Operator must be installed separately using the pmm-ha-dependencies chart
## before installing this chart. This section configures the VictoriaMetrics resources (not the operator itself).
## 
## Installation order:
##   1. helm install pmm-operators charts/pmm-ha-dependencies -n <namespace>
##   2. helm install pmm charts/pmm-ha -n <namespace>
## 
## This configuration uses VictoriaMetrics Operator CRDs (VMCluster, VMAuth, VMAgent, VMStorage)
victoriaMetrics:
  ## @param victoriaMetrics.enabled Enable VictoriaMetrics components via operator
  enabled: true
  
  ## @param victoriaMetrics.replicationFactor Replication factor for VMCluster
  ## Defines how many copies of data to store across vmstorage nodes
  replicationFactor: 2
  
  ## VMSelect configuration - handles read queries
  vmselect:
    ## @param victoriaMetrics.vmselect.replicaCount Number of vmselect replicas
    replicaCount: 2
    ## @param victoriaMetrics.vmselect.resources Resource requests and limits for vmselect
    resources:
      requests:
        memory: "512Mi"
        cpu: "200m"
      limits:
        memory: "2Gi"
        cpu: "1"
    ## @param victoriaMetrics.vmselect.affinity Pod affinity configuration for vmselect
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vmselect
    ## @param victoriaMetrics.vmselect.tolerations Tolerations for vmselect pods
    tolerations: []
    ## @param victoriaMetrics.vmselect.nodeSelector Node selector for vmselect pods
    nodeSelector: {}
    ## @param victoriaMetrics.vmselect.podAnnotations Annotations for vmselect pods
    podAnnotations: {}
    ## @param victoriaMetrics.vmselect.extraArgs Extra arguments for vmselect
    extraArgs: {}
  
  ## VMInsert configuration - handles write requests
  vminsert:
    ## @param victoriaMetrics.vminsert.replicaCount Number of vminsert replicas
    replicaCount: 2
    ## @param victoriaMetrics.vminsert.resources Resource requests and limits for vminsert
    resources:
      requests:
        memory: "512Mi"
        cpu: "200m"
      limits:
        memory: "2Gi"
        cpu: "1"
    ## @param victoriaMetrics.vminsert.affinity Pod affinity configuration for vminsert
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vminsert
    ## @param victoriaMetrics.vminsert.tolerations Tolerations for vminsert pods
    tolerations: []
    ## @param victoriaMetrics.vminsert.nodeSelector Node selector for vminsert pods
    nodeSelector: {}
    ## @param victoriaMetrics.vminsert.podAnnotations Annotations for vminsert pods
    podAnnotations: {}
    ## @param victoriaMetrics.vminsert.extraArgs Extra arguments for vminsert
    extraArgs:
      maxLabelsPerTimeseries: "50"
  
  ## VMAuth configuration - authentication proxy
  ## Credentials are read from pmm-secret (VMAGENT_remoteWrite_basicAuth_username/password)
  ## If pmm-secret doesn't exist, credentials are auto-generated (see templates/secret.yaml)
  ## Default username: victoriametrics_pmm
  ## Default password: randomly generated 32-character string
  vmauth:
    ## @param victoriaMetrics.vmauth.enabled Enable VMAuth for authentication
    enabled: true
    ## @param victoriaMetrics.vmauth.replicaCount Number of vmauth replicas
    replicaCount: 2
    ## @param victoriaMetrics.vmauth.resources Resource requests and limits for vmauth
    resources:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "500m"
    ## @param victoriaMetrics.vmauth.affinity Pod affinity configuration for vmauth
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vmauth
    ## @param victoriaMetrics.vmauth.tolerations Tolerations for vmauth pods
    tolerations: []
    ## @param victoriaMetrics.vmauth.nodeSelector Node selector for vmauth pods
    nodeSelector: {}
    ## @param victoriaMetrics.vmauth.podAnnotations Annotations for vmauth pods
    podAnnotations: {}
    ## @param victoriaMetrics.vmauth.extraArgs Extra arguments for vmauth
    extraArgs: {}
  
  ## VMAgent configuration - scrapes metrics and forwards to VictoriaMetrics
  ## VMAgent scrapes metrics from various sources and forwards them to VMAuth
  ## Credentials are read from pmm-secret (VMAGENT_remoteWrite_basicAuth_username/password)
  vmagent:
    ## @param victoriaMetrics.vmagent.enabled Enable VMAgent for metrics scraping
    enabled: true
    ## @param victoriaMetrics.vmagent.replicaCount Number of vmagent replicas
    replicaCount: 2
    ## @param victoriaMetrics.vmagent.resources Resource requests and limits for vmagent
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "1Gi"
        cpu: "500m"
    ## @param victoriaMetrics.vmagent.affinity Pod affinity configuration for vmagent
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vmagent
    ## @param victoriaMetrics.vmagent.tolerations Tolerations for vmagent pods
    tolerations: []
    ## @param victoriaMetrics.vmagent.nodeSelector Node selector for vmagent pods
    nodeSelector: {}
    ## @param victoriaMetrics.vmagent.podAnnotations Annotations for vmagent pods
    podAnnotations: {}
    ## @param victoriaMetrics.vmagent.extraArgs Extra arguments for vmagent
    extraArgs: {}
    ## @param victoriaMetrics.vmagent.scrapeConfigs Scrape configurations for VMAgent
    ## Can be used to configure Kubernetes service discovery, static targets, etc.
    ## Example:
    ## scrapeConfigs: []
    scrapeConfigs: []
  
  ## VMStorage configuration - stores metrics data
  vmstorage:
    ## @param victoriaMetrics.vmstorage.replicaCount Number of vmstorage replicas
    replicaCount: 3
    ## @param victoriaMetrics.vmstorage.retentionPeriod Data retention period
    retentionPeriod: "90d"
    ## @param victoriaMetrics.vmstorage.resources Resource requests and limits for vmstorage
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "4Gi"
        cpu: "2"
    ## @param victoriaMetrics.vmstorage.storageSize Size of persistent volume for each vmstorage pod
    storageSize: 50Gi
    ## @param victoriaMetrics.vmstorage.storageClassName Storage class for VictoriaMetrics persistent volumes
    ## If empty, uses the cluster's default storage class
    storageClassName: ""
    ## @param victoriaMetrics.vmstorage.affinity Pod affinity configuration for vmstorage
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vmstorage
    ## @param victoriaMetrics.vmstorage.tolerations Tolerations for vmstorage pods
    tolerations: []
    ## @param victoriaMetrics.vmstorage.nodeSelector Node selector for vmstorage pods
    nodeSelector: {}
    ## @param victoriaMetrics.vmstorage.podAnnotations Annotations for vmstorage pods
    podAnnotations: {}
    ## @param victoriaMetrics.vmstorage.extraArgs Extra arguments for vmstorage
    extraArgs: {}
