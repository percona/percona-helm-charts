## @section Percona Monitoring and Management (PMM) parameters
## Default values for PMM.
## This is a YAML-formatted file.
## Declare variables to be passed into your templates.

## PMM image version
## ref: https://hub.docker.com/r/percona/pmm-server/tags
## @param image.repository PMM image repository
## @param image.pullPolicy PMM image pull policy
## @param image.tag PMM image tag (immutable tags are recommended)
## @param image.imagePullSecrets Global Docker registry secret names as an array
##
image:
  repository: perconalab/pmm-server
  # repository: percona/pmm-server
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "3-dev-latest"
  # tag: "latest"
  imagePullSecrets: []

## PMM environment variables
## ref: https://docs.percona.com/percona-monitoring-and-management/setting-up/server/docker.html#environment-variables
##
pmmEnv:
  ## @param pmmEnv.DISABLE_UPDATES Disables a periodic check for new PMM versions as well as ability to apply upgrades using the UI (need to be disabled in k8s environment as updates rolled with helm/container update)
  ##
  DISABLE_UPDATES: "1"
  PMM_DEBUG: "0"
  # PMM_ACTIVE_IP: "pmmhak8s-0.monitoring-service.pmmhak8s.svc.cluster.local"
  # PMM_ACTIVE_NODE_ID: "pmmhak8s-0"
  PMM_DISABLE_BUILTIN_CLICKHOUSE: "1"
  PMM_DISABLE_BUILTIN_POSTGRES: "1"
  # PMM_CLICKHOUSE_ADDR: "pmmhak8s-clickhouse.pmmhak8s.svc.cluster.local:9000"
  PMM_CLICKHOUSE_DATABASE: "pmm"
  PMM_CLICKHOUSE_BLOCK_SIZE: "10000"
  PMM_CLICKHOUSE_POOL_SIZE: "2"
  # PMM_CLICKHOUSE_USER: "chuser"
  # PMM_CLICKHOUSE_PASSWORD: "chpass"
  # PMM_POSTGRES_ADDR: "pmmhak8s-pg-db-ha.pmmhak8s.svc.cluster.local:5432"
  PMM_POSTGRES_SSL_MODE: "require"
  # GF_DATABASE_URL: "postgres://$(GF_USERNAME):$(GF_PASSWORD)@pmmhak8s-pg-db-ha.pmmhak8s.svc.cluster.local:5432/grafana"
  GF_DATABASE_SSL_MODE: "require"
  # PMM_VM_URL: "http://vmadmin:vmpass@pmmhak8s-victoria-metrics-cluster-vmauth.pmmhak8s.svc.cluster.local:8427/"
  # VMAGENT_remoteWrite_basicAuth_username: "vmadmin"
  # VMAGENT_remoteWrite_basicAuth_password: "vmpass"
  PMM_TEST_HA_ENABLE: "1"
  PMM_TEST_HA_BOOTSTRAP: "1"
  PMM_TEST_HA_GOSSIP_PORT: "9096"
  PMM_TEST_HA_RAFT_PORT: "9097"
  PMM_TEST_HA_GRAFANA_GOSSIP_PORT: "9094"
  # PMM_TEST_HA_PEERS: "pmmhak8s-0.monitoring-service.pmmhak8s.svc.cluster.local,pmmhak8s-1.monitoring-service.pmmhak8s.svc.cluster.local,pmmhak8s-2.monitoring-service.pmmhak8s.svc.cluster.local"

#  optional variables to integrate Grafana with internal iDP, see also secret part
#  GF_AUTH_GENERIC_OAUTH_ENABLED: 'true'
#  GF_AUTH_GENERIC_OAUTH_SCOPES: ''
#  GF_AUTH_GENERIC_OAUTH_AUTH_URL: ''
#  GF_AUTH_GENERIC_OAUTH_TOKEN_URL: ''
#  GF_AUTH_GENERIC_OAUTH_API_URL: ''
#  GF_AUTH_GENERIC_OAUTH_ALLOWED_DOMAINS: ''

## @param pmmResources optional [Resources](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) requested for [PMM container](https://docs.percona.com/percona-monitoring-and-management/setting-up/server/index.html#set-up-pmm-server)
    ##  pmmResources:
    ##    requests:
    ##      memory: "32Gi"
    ##      cpu: "8"
    ##    limits:
    ##      memory: "64Gi"
    ##      cpu: "32"
pmmResources: {}

## Readiness probe Config
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
## @param readyProbeConf.initialDelaySeconds  Number of seconds after the container has started before readiness probes is initiated
## @param readyProbeConf.periodSeconds How often (in seconds) to perform the probe
## @param readyProbeConf.failureThreshold When a probe fails, Kubernetes will try failureThreshold times before giving up
##
readyProbeConf:
  initialDelaySeconds: 1
  periodSeconds: 5
  failureThreshold: 6

## @section PMM secrets
##
secret:
  ## @param secret.name Defines the name of the k8s secret that holds passwords and other secrets
  ##
  name: pmm-secret
  ## @param secret.annotations -- Secret annotations configuration
  annotations: {}
  ## @param secret.create If true then secret will be generated by Helm chart. Otherwise it is expected to be created by user.
  ##
  create: false
  ## @param secret.pmm_password Initial PMM password - it changes only on the first deployment, ignored if PMM was already provisioned and just restarted. If PMM admin password is not set, it will be generated.
  ## E.g.
  ## pmm_password: admin
  ##
  ## To get password execute `kubectl get secret pmm-secret -o jsonpath='{.data.PMM_ADMIN_PASSWORD}' | base64 --decode`
  ##
  pmm_password: ""
  ##
  # GF_AUTH_GENERIC_OAUTH_CLIENT_ID optional client ID to integrate Grafana with internal iDP, requires other env defined as well under pmmEnv
  # GF_AUTH_GENERIC_OAUTH_CLIENT_ID:
  # GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET optional secret to integrate Grafana with internal iDP, requires other env defined as well under pmmEnv
  # GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET:

## @param certs Optional certificates, if not provided PMM would use generated self-signed certificates,
##   please provide your own signed ssl certificates like this in base 64 format:
## certs:
  ## name: pmm-certs
  ## files:
  ##  certificate.crt:
  ##  certificate.key:
  ##  ca-certs.pem:
  ##  dhparam.pem:
  ##  certificate.conf:
certs: {}

## @section PMM network configuration
## Service configuration
##
service:
  ## @param service.name Service name that is dns name monitoring services would send data to. `monitoring-service` used by default by pmm-client in Percona operators.
  ##
  name: monitoring-service
  ## @param service.type Kubernetes Service type
  ##
  type: "ClusterIP"

  ## Ports 443 and/or 80
  ##
  ports:
    ## @param service.ports[0].port https port number
    - port: 8443
      ## @param service.ports[0].targetPort target port to map for statefulset and ingress
      targetPort: https
      ## @param service.ports[0].protocol protocol for https
      protocol: TCP
      ## @param service.ports[0].name port name
      name: https
    ## @param service.ports[1].port http port number
    - port: 8080
      ## @param service.ports[1].targetPort target port to map for statefulset and ingress
      targetPort: http
      ## @param service.ports[1].protocol protocol for http
      protocol: TCP
      ## @param service.ports[1].name port name
      name: http

# haservice:
#  type: "ClusterIP"
#  name: "pmmha"

replicas: 3

## Ingress controller configuration
##
ingress:
  ## @param ingress.enabled -- Enable ingress controller resource
  enabled: false
  ## @param ingress.nginxInc -- Using ingress controller from NGINX Inc
  nginxInc: false
  ## @param ingress.annotations -- Ingress annotations configuration
  annotations: {}
    ## kubernetes.io/ingress.class: nginx
    ## kubernetes.io/tls-acme: "true"
    ###  nginx proxy to https
    ## nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
  ## @param ingress.community.annotations -- Ingress annotations configuration for community managed ingress (nginxInc = false)
  community:
    annotations: {}
      ## kubernetes.io/ingress.class: nginx
      ## kubernetes.io/tls-acme: "true"
  ## @param ingress.ingressClassName -- Sets the ingress controller class name to use.
  ingressClassName: ""

  ## Ingress resource hostnames and path mappings
  hosts:
    ## @param ingress.hosts[0].host hostname
    - host: chart-example.local
    ## @param ingress.hosts[0].paths path mapping
      paths: []

  ## @param ingress.pathType -- How ingress paths should be treated.
  pathType: Prefix

  ## @param ingress.tls -- Ingress TLS configuration
  tls: []
  ##  - secretName: chart-example-tls
  ##    hosts:
  ##      - chart-example.local

## @section PMM storage configuration
## Claiming storage for PMM using Persistent Volume Claims (PVC)
## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
##
storage:
  ## @param storage.name name of PVC
  name: pmm-storage
  ## @param storage.storageClassName optional PMM data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClassName: ""
  ##
  ## @param storage.size size of storage [depends](https://docs.percona.com/percona-monitoring-and-management/setting-up/server/index.html#set-up-pmm-server) on number of monitored services and data retention
  ##
  size: 10Gi
  ##
  ## @param storage.dataSource VolumeSnapshot to start from
  ##
  dataSource: {}
    ## name: before-vX.Y.Z-upgrade
    ## kind: VolumeSnapshot
    ## apiGroup: snapshot.storage.k8s.io
  ##
  ## @param storage.selector select existing PersistentVolume
  ##
  selector: {}
  ##   matchLabels:
  ##     release: "stable"
  ##   matchExpressions:
  ##     - key: environment
  ##       operator: In
  ##       values:
  ##         - dev

## @section PMM kubernetes configurations
## @param nameOverride String to partially override common.names.fullname template with a string (will prepend the release name)
##
nameOverride: ""

## @param extraLabels Labels to add to all deployed objects
##
extraLabels: {}

## Pods Service Account
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
## @param serviceAccount.create Specifies whether a ServiceAccount should be created
## @param serviceAccount.annotations Annotations for service account. Evaluated as a template. Only used if `create` is `true`.
## @param serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
##
serviceAccount:
  create: true
  annotations: {}
  name: "pmm-service-account"

## @param podAnnotations Pod annotations
## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
##
podAnnotations: {}

## @param podSecurityContext Configure Pods Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
## E.g
podSecurityContext:
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000

## @param securityContext Configure Container Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
## securityContext.capabilities The capabilities to add/drop when running containers
## securityContext.runAsUser Set pmm containers' Security Context runAsUser
## securityContext.runAsNonRoot Set pmm container's Security Context runAsNonRoot
## E.g.
## securityContext:
  ## capabilities:
  ##   drop:
  ##   - ALL
  ## readOnlyRootFilesystem: true
  ## runAsNonRoot: true
  ## runAsUser: 1000
securityContext: {}


## @param nodeSelector Node labels for pod assignment
## Ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}

## @param tolerations Tolerations for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations: []

## @param affinity Affinity for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
##
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: pmm
          topologyKey: kubernetes.io/hostname

## @param extraVolumeMounts Optionally specify extra list of additional volumeMounts
##
extraVolumeMounts: []
## @param extraVolumes Optionally specify extra list of additional volumes
##
extraVolumes: []

global:
  pmm_ha_0: "{{ .Release.Name }}-0.monitoring-service.{{ .Release.Namespace }}.svc.cluster.local"
  pmm_ha_1: "{{ .Release.Name }}-1.monitoring-service.{{ .Release.Namespace }}.svc.cluster.local"
  pmm_ha_2: "{{ .Release.Name }}-2.monitoring-service.{{ .Release.Namespace }}.svc.cluster.local"

# Subchart scheduling overrides to spread replicas across nodes
clickhouse:
  zookeeper:
    podAntiAffinityPreset: soft
  podAntiAffinityPreset: soft
  distributeReplicasByZone: true

haproxy:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: haproxy
  config: |
      global
          log stdout    local0 debug
          log stdout    local1 info
          log stdout    local2 info
          daemon
      
      defaults
          log     global
          mode    http
          option  httplog
          option  dontlognull
          timeout connect 5000
          timeout client  50000
          timeout server  50000
      
      frontend http_front
          bind *:80
          default_backend http_back    
     
      frontend https_front
          bind *:443 ssl crt /etc/haproxy/certs/pmm.pem
          default_backend https_back

      backend http_back
          option httpchk
          http-check send meth GET uri /v1/server/leaderHealthCheck ver HTTP/1.1 hdr Host www
          http-check expect status 200
          server pmm-server-active-http  {{ tpl .Values.global.pmm_ha_0 . }}:8080 check
          server pmm-server-passive-http {{ tpl .Values.global.pmm_ha_1 . }}:8080 check backup
          server pmm-server-passive-2-http {{ tpl .Values.global.pmm_ha_2 . }}:8080 check backup

      backend https_back
          option httpchk
          http-check send meth GET uri /v1/server/leaderHealthCheck ver HTTP/1.1 hdr Host www
          http-check expect status 200
          server pmm-server-active-https {{ tpl .Values.global.pmm_ha_0 . }}:8443 check ssl verify none
          server pmm-server-passive-https {{ tpl .Values.global.pmm_ha_1 . }}:8443 check ssl verify none
          server pmm-server-passive-2-https {{ tpl .Values.global.pmm_ha_2 . }}:8443 check ssl verify none

pg-db:
  instances:
    - name: instance1
      replicas: 3
      dataVolumeClaimSpec:
        # storageClassName: ""  # optional; inherit default if omitted
        resources:
          requests:
            storage: 5Gi
  proxy:
    pgBouncer:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    postgres-operator.crunchydata.com/role: pgbouncer

victoria-metrics-cluster:
  vmselect:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values: ["victoria-metrics-cluster"]
                  - key: app
                    operator: In
                    values: ["vmselect"]
  vminsert:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values: ["victoria-metrics-cluster"]
                  - key: app
                    operator: In
                    values: ["vminsert"]
  vmauth:
    config:
      unauthorized_user: {}
      users:
        - username: "vmuser"
          password: "vmpass"
          url_map:
            - src_paths:
                - "/api/v1/write"
              url_prefix: "http://{{ .Release.Name }}-victoria-metrics-cluster-vminsert.{{ .Release.Namespace }}.svc.cluster.local:8480/insert/1/prometheus/"
            - src_paths:
                - "/api/v1/query"
                - "/api/v1/query_range"
                - "/api/v1/series"
                - "/api/v1/labels"
                - "/api/v1/label/.+/values"
              url_prefix: "http://{{ .Release.Name }}-victoria-metrics-cluster-vmselect.{{ .Release.Namespace }}.svc.cluster.local:8481/select/1/prometheus/"
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values: ["victoria-metrics-cluster"]
                  - key: app
                    operator: In
                    values: ["vmauth"]
  vmstorage:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values: ["victoria-metrics-cluster"]
                  - key: app
                    operator: In
                    values: ["vmstorage"]
