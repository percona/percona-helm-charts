## @section Percona Monitoring and Management (PMM) parameters
## Default values for PMM.
## This is a YAML-formatted file.
## Declare variables to be passed into your templates.

## PMM image version
## ref: https://hub.docker.com/r/percona/pmm-server/tags
## @param image.repository PMM image repository
## @param image.pullPolicy PMM image pull policy
## @param image.tag PMM image tag (immutable tags are recommended)
## @param image.imagePullSecrets Global Docker registry secret names as an array
##
image:
  repository: perconalab/pmm-server-fb
  # repository: percona/pmm-server
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "PR-4079-ff16373"
  imagePullSecrets: []

## PMM environment variables
## ref: https://docs.percona.com/percona-monitoring-and-management/3/install-pmm/install-pmm-server/deployment-options/docker/env_var.html
##
pmmEnv:
  ## @param pmmEnv.PMM_ENABLE_UPDATES needs to be disabled in k8s environment as updates are rolled out via helm/container update
  ##
  PMM_ENABLE_UPDATES: "0"
  PMM_DEBUG: "0"
  PMM_DISABLE_BUILTIN_CLICKHOUSE: "1"
  PMM_DISABLE_BUILTIN_POSTGRES: "1"
  # PMM_CLICKHOUSE_ADDR: "{{ .Release.Name }}-clickhouse.{{ .Release.Namespace }}.svc.cluster.local:9000"
  PMM_CLICKHOUSE_DATABASE: "pmm"
  PMM_CLICKHOUSE_IS_CLUSTER: "1"
  # PMM_CLICKHOUSE_CLUSTER_NAME is set directly in statefulset.yaml
  # PMM_CLICKHOUSE_USER: "clickhouse_pmm"
  # PMM_CLICKHOUSE_PASSWORD: "chpass"
  # PMM_POSTGRES_ADDR: "pmmhak8s-pg-db-ha.pmmhak8s.svc.cluster.local:5432"
  # PMM_POSTGRES_SSL_MODE: "require"
  # GF_DATABASE_URL: "postgres://$(GF_USERNAME):$(GF_PASSWORD)@:5432/grafana"
  GF_DATABASE_SSL_MODE: "require"
  # PMM_VM_URL: "http://:@:8427/"
  # VMAGENT_remoteWrite_basicAuth_username: ""
  # VMAGENT_remoteWrite_basicAuth_password: ""
  PMM_HA_ENABLE: "1"
  PMM_HA_BOOTSTRAP: "1"
  PMM_HA_GOSSIP_PORT: "9096"
  PMM_HA_RAFT_PORT: "9097"
  PMM_HA_GRAFANA_GOSSIP_PORT: "9094"
  # PMM_HA_PEERS: "{{ include "pmm.haPeers" . }}"
  GF_ANALYTICS_CHECK_FOR_UPDATES: "0"
  GF_ANALYTICS_ENABLE: "0"
  GF_ANALYTICS_REPORTING_ENABLE: "0"
  GF_REPORTING_ENABLE: "0"
  GF_NEWS_NEWS_FEED_ENABLE: "0"
  GF_SECURITY_DISABLE_GRAVATAR: "1"
  GF_LOG_LEVEL: "info"
  PMM_ENABLE_TELEMETRY: "0"
  PMM_ENABLE_BACKUP_MANAGEMENT: "1"
  PMM_ENABLE_ALERTING: "1"
  PMM_ENABLE_ACCESS_CONTROL: "1"

#  optional variables to integrate Grafana with internal iDP, see also secret part
#  GF_AUTH_GENERIC_OAUTH_ENABLED: "1"
#  GF_AUTH_GENERIC_OAUTH_SCOPES: ''
#  GF_AUTH_GENERIC_OAUTH_AUTH_URL: ''
#  GF_AUTH_GENERIC_OAUTH_TOKEN_URL: ''
#  GF_AUTH_GENERIC_OAUTH_API_URL: ''
#  GF_AUTH_GENERIC_OAUTH_ALLOWED_DOMAINS: ''

## @param pmmResources optional [Resources](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) requested for [PMM container](https://docs.percona.com/percona-monitoring-and-management/setting-up/server/index.html#set-up-pmm-server)
    ##  pmmResources:
    ##    requests:
    ##      memory: "32Gi"
    ##      cpu: "8"
    ##    limits:
    ##      memory: "64Gi"
    ##      cpu: "32"
pmmResources: {}

## Readiness probe Config
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
## @param readyProbeConf.initialDelaySeconds  Number of seconds after the container has started before readiness probes is initiated
## @param readyProbeConf.periodSeconds How often (in seconds) to perform the probe
## @param readyProbeConf.failureThreshold When a probe fails, Kubernetes will try failureThreshold times before giving up
##
readyProbeConf:
  initialDelaySeconds: 1
  periodSeconds: 5
  failureThreshold: 6

## @section PMM secrets
##
secret:
  ## @param secret.name Defines the name of the k8s secret that holds passwords and other secrets
  ##
  name: pmm-secret
  ## @param secret.annotations -- Secret annotations configuration
  annotations: {}
  ## @param secret.create If true then secret will be generated by Helm chart. Otherwise it is expected to be created by user.
  ##
  create: false
  ## @param secret.pmm_password Initial PMM password - it changes only on the first deployment, ignored if PMM was already provisioned and just restarted. If PMM admin password is not set, it will be generated.
  ## E.g.
  ## pmm_password: admin
  ##
  ## To get password execute `kubectl get secret pmm-secret -o jsonpath='{.data.PMM_ADMIN_PASSWORD}' | base64 --decode`
  ##
  pmm_password: ""
  ##
  ## @param secret.clickhouse_user ClickHouse user. If not set, defaults to "clickhouse_pmm"
  ## When secret.create: true and this is empty, "clickhouse_pmm" is used.
  ## If creating the secret manually (secret.create: false), this field is ignored.
  ##
  ## To get username after install:
  ##   kubectl get secret pmm-secret -o jsonpath='{.data.PMM_CLICKHOUSE_USER}' | base64 --decode
  ## E.g.
  ## clickhouse_user: clickhouse_user
  ##
  clickhouse_user: ""
  ##
  ## @param secret.clickhouse_password ClickHouse password. If not set, it will be auto-generated
  ## When secret.create: true and this is empty, a random 32-character password is generated.
  ## If creating the secret manually (secret.create: false), this field is ignored.
  ##
  ## To get password after install:
  ##   kubectl get secret pmm-secret -o jsonpath='{.data.PMM_CLICKHOUSE_PASSWORD}' | base64 --decode
  ## E.g.
  ## clickhouse_password: my_secure_password
  ##
  clickhouse_password: ""
  ##
  ## @param secret.victoriametrics_user VictoriaMetrics user. If not set, defaults to "victoriametrics_pmm"
  ## When secret.create: true and this is empty, "victoriametrics_pmm" is used.
  ## If creating the secret manually (secret.create: false), this field is ignored.
  ##
  ## To get username after install:
  ##   kubectl get secret pmm-secret -o jsonpath='{.data.VMAGENT_remoteWrite_basicAuth_username}' | base64 --decode
  ## E.g.
  ## victoriametrics_user: my_victoriametrics_user
  ##
  victoriametrics_user: ""
  ##
  ## @param secret.victoriametrics_password VictoriaMetrics password. If not set, it will be auto-generated
  ## When secret.create: true and this is empty, a random 32-character password is generated.
  ## If creating the secret manually (secret.create: false), this field is ignored.
  ##
  ## To get password after install:
  ##   kubectl get secret pmm-secret -o jsonpath='{.data.VMAGENT_remoteWrite_basicAuth_password}' | base64 --decode
  ## E.g.
  ## victoriametrics_password: my_secure_password
  ##
  victoriametrics_password: ""
  ##
  # GF_AUTH_GENERIC_OAUTH_CLIENT_ID optional client ID to integrate Grafana with internal iDP, requires other env defined as well under pmmEnv
  # GF_AUTH_GENERIC_OAUTH_CLIENT_ID:
  # GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET optional secret to integrate Grafana with internal iDP, requires other env defined as well under pmmEnv
  # GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET:

## @param certs Optional certificates, if not provided PMM would use generated self-signed certificates,
##   please provide your own signed ssl certificates like this in base 64 format:
## certs:
  ## name: pmm-certs
  ## files:
  ##  certificate.crt:
  ##  certificate.key:
  ##  ca-certs.pem:
  ##  dhparam.pem:
  ##  certificate.conf:
certs: {}

## @section PMM network configuration
## Service configuration
##
service:
  ## @param service.name Service name that is dns name monitoring services would send data to. `monitoring-service` used by default by pmm-client in Percona operators.
  ##
  name: monitoring-service
  ## @param service.type Kubernetes Service type
  ##
  type: "ClusterIP"

  ## Ports 8443 for https
  ##
  ports:
    ## @param service.ports[0].port https port number
    - port: 8443
      ## @param service.ports[0].targetPort target port to map for statefulset and ingress
      targetPort: https
      ## @param service.ports[0].protocol protocol for https
      protocol: TCP
      ## @param service.ports[0].name port name
      name: https

# haservice:
#  type: "ClusterIP"
#  name: "pmmha"

replicas: 3

## Ingress controller configuration
##
ingress:
  ## @param ingress.enabled -- Enable ingress controller resource
  enabled: false
  ## @param ingress.nginxInc -- Using ingress controller from NGINX Inc
  nginxInc: false
  ## @param ingress.annotations -- Ingress annotations configuration
  annotations: {}
    ## kubernetes.io/ingress.class: nginx
    ## kubernetes.io/tls-acme: "true"
    ###  nginx proxy to https
    ## nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
  ## @param ingress.community.annotations -- Ingress annotations configuration for community managed ingress (nginxInc = false)
  community:
    annotations: {}
      ## kubernetes.io/ingress.class: nginx
      ## kubernetes.io/tls-acme: "true"
  ## @param ingress.ingressClassName -- Sets the ingress controller class name to use.
  ingressClassName: ""

  ## Ingress resource hostnames and path mappings
  hosts:
    ## @param ingress.hosts[0].host hostname
    - host: chart-example.local
    ## @param ingress.hosts[0].paths path mapping
      paths: []

  ## @param ingress.pathType -- How ingress paths should be treated.
  pathType: Prefix

  ## @param ingress.tls -- Ingress TLS configuration
  tls: []
  ##  - secretName: chart-example-tls
  ##    hosts:
  ##      - chart-example.local

## @section PMM storage configuration
## Claiming storage for PMM using Persistent Volume Claims (PVC)
## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
##
storage:
  ## @param storage.name name of PVC
  name: pmm-storage
  ## @param storage.storageClassName optional PMM data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClassName: ""
  ##
  ## @param storage.size size of storage [depends](https://docs.percona.com/percona-monitoring-and-management/setting-up/server/index.html#set-up-pmm-server) on number of monitored services and data retention
  ##
  size: 40Gi
  ##
  ## @param storage.dataSource VolumeSnapshot to start from
  ##
  dataSource: {}
    ## name: before-vX.Y.Z-upgrade
    ## kind: VolumeSnapshot
    ## apiGroup: snapshot.storage.k8s.io
  ##
  ## @param storage.selector select existing PersistentVolume
  ##
  selector: {}
  ##   matchLabels:
  ##     release: "stable"
  ##   matchExpressions:
  ##     - key: environment
  ##       operator: In
  ##       values:
  ##         - dev

## @section PMM kubernetes configurations
## @param nameOverride String to partially override common.names.fullname template with a string (will prepend the release name)
##
nameOverride: ""

## @param extraLabels Labels to add to all deployed objects
##
extraLabels: {}

## Pods Service Account
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
## @param serviceAccount.create Specifies whether a ServiceAccount should be created
## @param serviceAccount.annotations Annotations for service account. Evaluated as a template. Only used if `create` is `true`.
## @param serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
##
serviceAccount:
  create: true
  annotations: {}
  name: "pmm-service-account"

## @param podAnnotations Pod annotations
## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
##
podAnnotations: {}

## @param podSecurityContext Configure Pods Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
## E.g
podSecurityContext:
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000

## @param securityContext Configure Container Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
## securityContext.capabilities The capabilities to add/drop when running containers
## securityContext.runAsUser Set pmm containers' Security Context runAsUser
## securityContext.runAsNonRoot Set pmm container's Security Context runAsNonRoot
## E.g.
## securityContext:
  ## capabilities:
  ##   drop:
  ##   - ALL
  ## readOnlyRootFilesystem: true
  ## runAsNonRoot: true
  ## runAsUser: 1000
securityContext: {}


## @param nodeSelector Node labels for pod assignment
## Ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}

## @param tolerations Tolerations for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations: []

## @param affinity Affinity for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
##
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: pmm
          topologyKey: kubernetes.io/hostname

## @param extraVolumeMounts Optionally specify extra list of additional volumeMounts
##
extraVolumeMounts: []
## @param extraVolumes Optionally specify extra list of additional volumes
##
extraVolumes: []

clickhouse:
  # ClickHouse server image configuration
  image:
    repository: altinity/clickhouse-server
    tag: 25.3.6.10034.altinitystable-alpine
    pullPolicy: IfNotPresent
  
  # ClickHouse server resource configuration
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "4Gi"
      cpu: "2"
  
  cluster:
    # Cluster name restrictions: alphanumeric only, max 15 chars, no underscores
    name: pmmdb
    # Sharding and Replication setup
    shards: 1
    replicas: 3
  
  # Service configuration for sticky connections
  service:
    # Enable sticky service with session affinity
    enabled: true
    # Session affinity timeout in seconds (how long connections stay sticky)
    sessionAffinityTimeout: 10800  # 3 hours
  
  # ClickHouse Keeper configuration
  keeper:
    # ClickHouse Keeper image configuration
    image:
      repository: clickhouse/clickhouse-keeper
      tag: "25.3.6.56"
      pullPolicy: IfNotPresent
    # ClickHouse Keeper resource configuration
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "500m"
    # ClickHouse Keeper cluster configuration
    cluster:
      # Keeper cluster name (follows same restrictions as ClickHouse cluster)
      # Regex: ^[a-zA-Z0-9-]{0,15}$
      name: keeper
    # Number of Keeper nodes (should be odd number: 1, 3, 5, etc.)
    replicasCount: 3
    # Persistent storage configuration for Keeper data
    storage:
      # Size of persistent volume for each Keeper pod
      size: 5Gi
      # Storage class for Keeper persistent volumes
      # If empty, uses the cluster's default storage class
      storageClassName: ""
  
  # Persistent storage configuration for ClickHouse data
  storage:
    # Size of persistent volume for each ClickHouse pod
    size: 20Gi
    # Storage class for ClickHouse persistent volumes
    # If empty, uses the cluster's default storage class
    # storageClassName: ""
    storageClassName: ""
  
  # PMM application user configuration
  # User credentials are read from the pmm-secret at chart installation time
  users:
    # Network access configuration for ClickHouse user
    networks:
      ip:
        # Allow access from Kubernetes pod networks and HAProxy
        - 10.0.0.0/8
        - 172.16.0.0/12
        - 192.168.0.0/16

haproxy:
  config: ""
  configmap:
    enabled: false
  # HAProxy dependency configuration
  enabled: true
  # Number of HAProxy replicas
  replicaCount: 3
  
  # Pod annotations - increment the version value to trigger HAProxy pod rolling when config changes
  podAnnotations:
    pmm.percona.com/config-version: "3"

  # Pod anti-affinity for HAProxy pods
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: haproxy
  # Init container to wait for PMM instances to be ready
  initContainers:
    - name: wait-for-pmm-ready
      image: "alpine:latest"
      command:
        - /bin/sh
        - /scripts/wait-for-pmm.sh
      volumeMounts:
        - name: init-script
          mountPath: /scripts
          readOnly: true
      resources:
        requests:
          cpu: 50m
          memory: 32Mi
        limits:
          cpu: 100m
          memory: 64Mi

  # Additional volumes for init container
  extraVolumes:
    - name: init-script
      configMap:
        name: pmm-ha-haproxy-init-script
        defaultMode: 0755
  # Disable default ConfigMap creation - we create our own
  configMap:
    enabled: false
  # Mount SSL certificate
  mountedSecrets:
    - volumeName: ssl-certificate
      secretName: haproxy-tls-secret
      mountPath: /etc/haproxy/certs
  
  # HAProxy monitoring and metrics configuration
  monitoring:
    # Enable HAProxy stats page
    stats:
      enabled: true
      port: 1024
      uri: /stats
      refresh: 30s
    # Enable Prometheus metrics export
    prometheus:
      enabled: true
      port: 1024
      path: /metrics
  
  # HAProxy security configuration
  security:
    # Enable HTTP to HTTPS redirect
    redirectToHttps: true

# HAProxy TLS configuration
tls:
  enabled: true
  # If true, skip secret creation entirely (you must manually create pemSecretName)
  # Set this to true if you want to manually manage haproxy-tls-secret
  manualSecret: false
  # If you have a k8s TLS secret (tls.crt/tls.key), Helm will convert it to PEM format
  # Leave empty to auto-generate self-signed certificate
  existingTLSSecret: ""            # e.g. "my-tls" (type=kubernetes.io/tls)
  # Name of the PEM secret that HAProxy will mount
  pemSecretName: "haproxy-tls-secret"
  cn: "pmm.local"
  dnsNames: 
    - "pmm.local"
    - "*.pmm.local" 
    - "localhost"
  validityDays: 1825  # 5 years
  # Force regeneration of the TLS secret (increment this value to force regeneration)
  forceRegenerate: 0

pg-db:
  instances:
    - name: instance1
      replicas: 3
      dataVolumeClaimSpec:
        # storageClassName: ""  # optional; inherit default if omitted
        resources:
          requests:
            storage: 5Gi
  proxy:
    pgBouncer:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    postgres-operator.crunchydata.com/role: pgbouncer
  patroni:
    dynamicConfiguration:
      postgresql:
        pg_hba:
          # Allow connections from private RFC1918 networks (common Kubernetes ranges)
          # 10.0.0.0/8 covers AWS and GCP
          # 172.16.0.0/12 covers Azure and Docker
          # 192.168.0.0/16 covers minikube and many on-prem setups
          - host all gfuser 10.0.0.0/8 md5
          - host all pmmuser 10.0.0.0/8 md5
          - host all gfuser 172.16.0.0/12 md5
          - host all pmmuser 172.16.0.0/12 md5
          - host all gfuser 192.168.0.0/16 md5
          - host all pmmuser 192.168.0.0/16 md5

victoria-metrics-cluster:
  vmselect:
    replicaCount: 2
    resources:
      requests:
        memory: "512Mi"
        cpu: "200m"
      limits:
        memory: "2Gi"
        cpu: "1"
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values: ["victoria-metrics-cluster"]
                  - key: app
                    operator: In
                    values: ["vmselect"]
  vminsert:
    replicaCount: 2
    resources:
      requests:
        memory: "512Mi"
        cpu: "200m"
      limits:
        memory: "2Gi"
        cpu: "1"
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values: ["victoria-metrics-cluster"]
                  - key: app
                    operator: In
                    values: ["vminsert"]
  vmauth:
    enabled: true
    replicaCount: 2
    # Custom Secret with auth configuration is created in templates/vmauth-config.yaml
    # The secret name follows: {{ .Release.Name }}-victoria-metrics-cluster-vmauth
    # The subchart evaluates this template using .helm.Release.Name from its context
    configSecretName: "{{ .helm.Release.Name }}-victoria-metrics-cluster-vmauth"
    resources:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "500m"
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values: ["victoria-metrics-cluster"]
                  - key: app
                    operator: In
                    values: ["vmauth"]
  vmstorage:
    replicaCount: 3
    retentionPeriod: "90d"
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "4Gi"
        cpu: "2"
    persistentVolume:
      enabled: true
      size: 50Gi
      # Storage class for VictoriaMetrics persistent volumes
      # If empty, uses the cluster's default storage class
      storageClassName: ""
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values: ["victoria-metrics-cluster"]
                  - key: app
                    operator: In
                    values: ["vmstorage"]

victoria-metrics-agent:
  # VMAgent configuration for scraping VictoriaMetrics cluster components
  # Note: The remoteWrite URL is automatically configured via ConfigMap (templates/vmagent-config.yaml)
  replicaCount: 2
  
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "1Gi"
      cpu: "500m"
  
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: victoria-metrics-agent
  
  extraArgs:
    envflag.enable: "true"
    envflag.prefix: "VM_"
    loggerFormat: "json"
  
  # Environment variables for basic auth credentials
  env:
    - name: VM_remoteWrite_basicAuth_username
      valueFrom:
        secretKeyRef:
          name: pmm-secret
          key: VMAGENT_remoteWrite_basicAuth_username
    - name: VM_remoteWrite_basicAuth_password
      valueFrom:
        secretKeyRef:
          name: pmm-secret
          key: VMAGENT_remoteWrite_basicAuth_password
  
  # Temporary placeholder - will be updated by manual patch or script
  remoteWrite:
    - url: http://placeholder:8427/api/v1/write
      basicAuth:
        username: placeholder
        password: placeholder
  
  # Scrape configuration for VictoriaMetrics cluster components
  config:
    global:
      scrape_interval: 15s
      scrape_timeout: 10s
    
    scrape_configs:
      # Scrape VMSelect metrics
      - job_name: 'vmselect'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            regex: '.*-vmselect.*'
            action: keep
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: 'http'
            action: keep
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
      
      # Scrape VMInsert metrics
      - job_name: 'vminsert'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            regex: '.*-vminsert.*'
            action: keep
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: 'http'
            action: keep
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
      
      # Scrape VMStorage metrics
      - job_name: 'vmstorage'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            regex: '.*-vmstorage.*'
            action: keep
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: 'http'
            action: keep
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
      
      # Scrape VMAuth metrics
      - job_name: 'vmauth'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            regex: '.*-vmauth.*'
            action: keep
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: 'http'
            action: keep
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
      
      # Scrape VMAgent itself
      - job_name: 'vmagent'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            regex: '.*-vmagent.*'
            action: keep
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: 'http'
            action: keep
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
